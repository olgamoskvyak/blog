{
  
    
        "post0": {
            "title": "Transformation equivariance loss",
            "content": "TODO . [ ] Add references to previous works | [ ] Add code with TE loss implementation? | . Discussion . Transformation equivariance (TE) loss is a constraint that can be applied to learning pixel-level labels such as segmentation, keypoint localization or parts detection. TE constraint ensures that the output of the model changes the same way as the input under spatial transformations. . For example, if the input image is rotated by 25 degrees then it is logical to expect that the output segmentation map should change consistently: . ![](my_icons/tf_equivariance.png) . ![](my_icons/fastai_logo.png) . . Important: In case of supervised training (ground truth annotations are available for the whole training set), the model attains transformation equivariance via data augmentation. TE loss is useful in unsupervised or self-supervised settings (without ground truth labels). . Note: For the clarity of the discussion, we narrow down to the object segmentation task. The same approach applies to instance segmentation and keypoint localization. . Note: We talk only about transformation in space and not color transformations. Color perturbations does not change detected objects in space. . Formula . Let us denote $F$ - the model, $T$ – transformation, $x$ – input image, $D$ - error function in the image space. Function $D$ depends on the application and can be a mean squared error or a perceptual loss. Transformation Equivariance (TE) loss is formulated as follows: . $$ L_{TE}(x) = D big(F(T(x)) - T(F(x)) big) $$ Differentiable transformations . Optimization of TE loss in the formula above requires to backpropagate through the transformation. While image transformations are provided in many augmentation pipelines such as Pytorch Torchvision transforms, Keras image data preprocessing, imgaug and OpenCV, these transformations do not support differentiation. . Fortunately, a new library Kornia comes to the resque to implement differentiable image transformations. Kornia replicates OpenCV functions on tensors and allows differentiation. . -- Inspired by OpenCV, this library is composed by a subset of packages containing operators that can be inserted within neural networks to train models to perform image transformations, ... that operate directly on tensors. . Now let&#39;s see how TE loss is computed on three examples. . Example 1: Dummy data on 1-layer CNN model . The first example is on dummy data and 1-layer convolutional model. . from tf_equivariance_loss import TfEquivarianceLoss import torch import torch.nn as nn . Get random dummy data: . x = torch.randn((4, 3, 64, 64)) . The demo model is a one layer model with three convolutional 1x1 filters (no bias) which is just a weighted combination of RGB layers of a colour image. The model satisfies transformation equivariance constraint just by its design. Let us check that the loss is zero. . model = nn.Sequential(nn.Conv2d(3, 1, 1, bias=False)) model . Sequential( (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1), bias=False) ) . Initialise Transformation Equivariance loss. This example implements rotation transformation $T$ with a mean squared error as the function $D$. . tf_equiv_loss = TfEquivarianceLoss( transform_type=&#39;rotation&#39;, consistency_type=&#39;mse&#39;, batch_size=4, max_angle=90, input_hw=(64, 64) ) . Compute loss on a batch of data $x$: . tf_equiv_loss.set_tf_matrices() # Compute model on input image fx = model(x) # Transform output tfx = tf_equiv_loss.transform(fx) # Transform input image tx = tf_equiv_loss.transform(x) # Compute model on the transformed image ftx = model(tx) loss = tf_equiv_loss(tfx, ftx) loss . tensor(6.8236e-16, grad_fn=&lt;MseLossBackward&gt;) . The computed loss is close to zero as expected for this kind of model. . Example 2: Real data on 1-layer CNN model . Second example uses images from a subset of Pascal VOC dataset (we select the smallest subset for demonstration purposes). . Data preparation in this example is a bit longer than for dummy data. Images are resized, converted to tensors and normalized. Segmentation maps are available for this dataset but they are not used for loss computations in our example (we still need to preprocess them to get torch tensors for data loader). . from torchvision.datasets import VOCSegmentation . import torchvision.transforms as T image_tf = T.Compose([ T.Resize((256, 256)), T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) ]) segm_tf = T.Compose([ T.Resize((256, 256)), T.ToTensor() ]) . dataset = VOCSegmentation( root=&#39;../&#39;, year=&#39;2007&#39;, image_set=&#39;test&#39;, download=True, transform=image_tf, target_transform=segm_tf, transforms=None, ) . Using downloaded and verified file: ../VOCtest_06-Nov-2007.tar . data_loader = torch.utils.data.DataLoader( dataset, batch_size=4, shuffle=True, num_workers=4, ) data_loader_iter = iter(data_loader) . model = nn.Sequential(nn.Conv2d(3, 1, 1, bias=False)) device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;) model.to(device) . Sequential( (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1), bias=False) ) . data_batch = next(data_loader_iter) # Take only images and discard annotations x = data_batch[0] . Note that we are not using ground truth annotation for computing loss in this example . tf_equiv_loss.set_tf_matrices() # Compute model on input image fx = model(x) # Transform output tfx = tf_equiv_loss.transform(fx) # Transform input image tx = tf_equiv_loss.transform(x) # Compute model on the transformed image ftx = model(tx) loss = tf_equiv_loss(tfx, ftx) loss . tensor(1.4272e-15, grad_fn=&lt;MseLossBackward&gt;) . import matplotlib.pyplot as plt %matplotlib inline . Lets plot input and rotated images and corresponding output model to check that our test model satisfies transformation equivariance constraint (consistent with rotation transformations). We can see that output of the model changes consistently with the image changes. . def unnormalize(batch_image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], use_gpu=False): &quot;&quot;&quot;Reverse normalization applied to image by transformations &quot;&quot;&quot; B = batch_image.shape[0] H = batch_image.shape[2] W = batch_image.shape[3] t_mean = torch.FloatTensor(mean).view(3, 1, 1). expand(3, H, W).contiguous().view(1, 3, H, W) t_std = torch.FloatTensor(std).view(3, 1, 1). expand(3, H, W).contiguous().view(1, 3, H, W) if use_gpu: t_mean = t_mean.cuda() t_std = t_std.cuda() batch_image_unnorm = batch_image * t_std.expand(B, 3, H, W) + t_mean.expand(B, 3, H, W) return batch_image_unnorm . fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(16, 16)) # Convert tensors to numpy, transpose channels and reverse normalization for display purposes x_display = unnormalize(x).numpy().transpose(0, 2, 3, 1) tx_display = unnormalize(tx).numpy().transpose(0, 2, 3, 1) fx_display = fx.squeeze().detach().cpu().numpy() ftx_display = ftx.squeeze().detach().cpu().numpy() for i in range(4): ax[0, i].imshow(x_display[i]) ax[0, i].axis(&#39;off&#39;) ax[1, i].imshow(fx_display[i], cmap=&#39;gray&#39;) ax[1, i].axis(&#39;off&#39;) ax[2, i].imshow(tx_display[i]) ax[2, i].axis(&#39;off&#39;) ax[3, i].imshow(ftx_display[i], cmap=&#39;gray&#39;) ax[3, i].axis(&#39;off&#39;) ax[0, 0].set_title(&#39;Input images&#39;, fontsize=20) ax[1, 0].set_title(&#39;Model output on input images&#39;, fontsize=20) ax[2, 0].set_title(&#39;Rotated images&#39;, fontsize=20) ax[3, 0].set_title(&#39;Model output on rotated images&#39;, fontsize=20) plt.tight_layout() . WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . As we see the loss is zero and the output of the model changes consistently when the input changes. . Example 3: Real data on pre-trained segmentation model . Data has been loaded in the previous example. We only need to download pre-trained segmentation model from torchvision library. . import torchvision import numpy as np . model = torchvision.models.segmentation.fcn_resnet50( pretrained=True, progress=True, num_classes=21, aux_loss=None ) model.to(device) . FCN( (backbone): IntermediateLayerGetter( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer2): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer3): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer4): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) ) (classifier): FCNHead( (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Dropout(p=0.1, inplace=False) (4): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1)) ) (aux_classifier): FCNHead( (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Dropout(p=0.1, inplace=False) (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1)) ) ) . tf_equiv_loss = TfEquivarianceLoss( transform_type=&#39;rotation&#39;, consistency_type=&#39;mse&#39;, batch_size=4, max_angle=45, input_hw=(256, 256) ) . data_batch = next(data_loader_iter) x = data_batch[0] . tf_equiv_loss.set_tf_matrices() # Compute model on input image fx = model(x)[&#39;out&#39;] # Transform output tfx = tf_equiv_loss.transform(fx) # Transform input image tx = tf_equiv_loss.transform(x) # Compute model on the transformed image ftx = model(tx)[&#39;out&#39;] loss = tf_equiv_loss(tfx, ftx) loss . tensor(2.9116, grad_fn=&lt;MseLossBackward&gt;) . def decode_segmap(image, nc=21): label_colors = np.array([(0, 0, 0), # 0=background # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128), # 6=bus, 7=car, 8=cat, 9=chair, 10=cow (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)]) r = np.zeros_like(image).astype(np.uint8) g = np.zeros_like(image).astype(np.uint8) b = np.zeros_like(image).astype(np.uint8) for l in range(0, nc): idx = image == l r[idx] = label_colors[l, 0] g[idx] = label_colors[l, 1] b[idx] = label_colors[l, 2] rgb = np.stack([r, g, b], axis=2) return rgb . Segmentation model inputs 21 channels for each image (one segmentation map for each class and one background map). We use helper function to decode segmentation maps. . fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(16, 16)) # Convert tensors to numpy, transpose channels and reverse normalization for display purposes x_display = unnormalize(x).numpy().transpose(0, 2, 3, 1) tx_display = unnormalize(tx).numpy().transpose(0, 2, 3, 1) # fx_display = fx.squeeze().detach().cpu().numpy() fx_display = torch.argmax(fx.squeeze(), dim=1).detach().cpu().numpy() # ftx_display = ftx.squeeze().detach().cpu().numpy() ftx_display = torch.argmax(ftx.squeeze(), dim=1).detach().cpu().numpy() for i in range(4): ax[0, i].imshow(x_display[i]) ax[0, i].axis(&#39;off&#39;) ax[1, i].imshow(decode_segmap(fx_display[i]), cmap=&#39;gray&#39;) ax[1, i].axis(&#39;off&#39;) ax[2, i].imshow(tx_display[i]) ax[2, i].axis(&#39;off&#39;) ax[3, i].imshow(decode_segmap(ftx_display[i]), cmap=&#39;gray&#39;) ax[3, i].axis(&#39;off&#39;) ax[0, 0].set_title(&#39;Input images&#39;, fontsize=20) ax[1, 0].set_title(&#39;Model output on input images&#39;, fontsize=20) ax[2, 0].set_title(&#39;Rotated images&#39;, fontsize=20) ax[3, 0].set_title(&#39;Model output on rotated images&#39;, fontsize=20) plt.tight_layout() . WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . The example shows that the loss value on the batch is greater than zero and printed results confirm that the current model is not transformation equivariant. .",
            "url": "https://olgamoskvyak.github.io/blog/unsupervised/losses/2021/03/25/transformation-equivariance-loss.html",
            "relUrl": "/unsupervised/losses/2021/03/25/transformation-equivariance-loss.html",
            "date": " • Mar 25, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://olgamoskvyak.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://olgamoskvyak.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}